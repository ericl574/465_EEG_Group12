{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Project Group 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Data and Performing Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here upto and including EEG Feature Extraction was taken from the sample example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Visual Studio Code\\Microsoft VS Code\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 113 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Get Current Working directory and append the data relative dir\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "Spicy = cwd + r\"\\Data\\Raw\\Spicy\"\n",
    "Salty = cwd + r\"\\Data\\Raw\\Salty\"\n",
    "\n",
    "#Hold file locations\n",
    "filesSpicy=[];\n",
    "filesSalty=[]; #add more later\n",
    "#populate file location arrays\n",
    "for file in os.listdir(Spicy):\n",
    "    if file.endswith('csv'):\n",
    "        filesSpicy.append(os.path.join(Spicy,file))\n",
    "for file in os.listdir(Salty):\n",
    "    if file.endswith('csv'):\n",
    "        filesSalty.append(os.path.join(Salty,file))\n",
    "#Test reading files by changing num\n",
    "num = 6;\n",
    "sample = pd.read_csv(filesSpicy[num])\n",
    "sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mini-Summary of Block\n",
    "print(f\">{len(filesSpicy)} files were added from the Spicy category\")\n",
    "print(f\">{len(filesSalty)} files were added from the Salty category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features generated by the Muse 2 headband:\")\n",
    "pd.DataFrame(sample.columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RAW Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Extract rows 21-25 from all files,\n",
    "#these are the only 5 features relevant for use in the EEG_feature_extraction function\n",
    "rowsSpicy=[];\n",
    "for f in filesSpicy:\n",
    "    for r in range(pd.read_csv(f).shape[0]):\n",
    "        rowsSpicy.append(pd.read_csv(f).iloc[r,[0,21,22,23,24,25]])\n",
    "rowsSalty=[];\n",
    "for f in filesSalty:\n",
    "    for r in range(pd.read_csv(f).shape[0]):\n",
    "        rowsSalty.append(pd.read_csv(f).iloc[r,[0,21,22,23,24,25]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to DataFrames\n",
    "data_Spicy = pd.DataFrame(rowsSpicy);\n",
    "original_Spicy = data_Spicy.copy();\n",
    "data_Spicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to DataFrames\n",
    "data_Salty = pd.DataFrame(rowsSalty);\n",
    "original_Salty = data_Salty.copy();\n",
    "data_Salty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick check of DataFrames\n",
    "print(f\"Spicy data size is: \\t{data_Spicy.shape}\",f\"\\nSalty data size is: \\t{data_Salty.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Empty Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove NaN values\n",
    "data_Spicy = data_Spicy.dropna()\n",
    "data_Salty = data_Salty.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Datetime Column to Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required for compatibility with EEG_feature_extraction function\n",
    "from datetime import datetime\n",
    "\n",
    "i = 0;\n",
    "for time in data_Spicy.iloc[:,0]:\n",
    "    tmstmp = datetime.strptime(str(time),'%Y-%m-%d %H:%M:%S.%f').timestamp()\n",
    "    data_Spicy.iat[i,0] = (tmstmp);\n",
    "i=i+1;\n",
    "\n",
    "i = 0;\n",
    "for time in data_Salty.iloc[:,0]:\n",
    "    tmstmp = datetime.strptime(str(time),'%Y-%m-%d %H:%M:%S.%f').timestamp()\n",
    "    data_Salty.iat[i,0] = (tmstmp);\n",
    "i=i+1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick Check\n",
    "data_Spicy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data to File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative STARTING POINT once data collection is finalized. This step was done to bypass having to run the previous section each time which would take a very long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savelocSpicy = cwd + r\"\\Data\\Preprocessed\\Spicy.csv\"\n",
    "savelocSalty = cwd + r\"\\Data\\Preprocessed\\Salty.csv\"\n",
    "\n",
    "if os.path.exists(savelocSpicy):\n",
    "    os.remove(savelocSpicy)\n",
    "if os.path.exists(savelocSalty):\n",
    "    os.remove(savelocSalty)\n",
    "data_Spicy.to_csv(savelocSpicy, mode='w', index = False)\n",
    "data_Salty.to_csv(savelocSalty, mode='w', index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEG Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eegFG import EEG_feature_extraction as FG\n",
    "\n",
    "#try various combinations of Nsamp and Perio\n",
    "Nsamp = 50;\n",
    "Perio = 6;\n",
    "\n",
    "xSpicy,ySpicy = FG.generate_feature_vectors_from_samples(file_path=savelocSpicy,\n",
    "                                                         nsamples=Nsamp,\n",
    "                                                         period=Perio,\n",
    "                                                         #state=data_Spicy.iloc[:,-1],\n",
    "                                                         slide_percent=0.05,\n",
    "                                                         remove_redundant=False,\n",
    "                                                         cols_to_ignore=None)\n",
    "xSpicy.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try various combinations of Nsamp and Perio\n",
    "Nsamp = 50;\n",
    "Perio = 5;\n",
    "\n",
    "xSalty,ySalty = FG.generate_feature_vectors_from_samples(file_path=savelocSalty,\n",
    "                                                         nsamples=Nsamp,\n",
    "                                                         period=Perio,\n",
    "                                                         #state=data_Salty.iloc[:,-1],\n",
    "                                                         slide_percent=0.06,\n",
    "                                                         remove_redundant=False,\n",
    "                                                         cols_to_ignore=None)\n",
    "xSalty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some quick checks\n",
    "X_Spicy = pd.DataFrame(np.real(xSpicy))\n",
    "X_Spicy.columns = np.hstack((['Timestamp'],ySpicy))\n",
    "X_Spicy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows',None,'display.max_columns',None):\n",
    "    display(pd.DataFrame(pd.DataFrame(X_Spicy).head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Salty = pd.DataFrame(np.real(xSalty))\n",
    "X_Salty.columns = np.hstack((['Timestamp'],ySalty))\n",
    "X_Salty.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Into Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
